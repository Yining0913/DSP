{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counterfactual explanations with one-hot encoded categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real world machine learning applications often handle data with categorical variables. Explanation methods which rely on perturbations of the input features need to make sure those perturbations are meaningful and capture the underlying structure of the data. This becomes tricky for categorical features. For instance random perturbations across possible categories or enforcing a ranking between categories based on frequency of occurrence in the training data do not capture this structure. Our method captures the relation between categories of a variable numerically through the context given by the other features in the data and/or the predictions made by the model. First it captures the pairwise distances between categories and then applies multi-dimensional scaling. More details about the method can be found in the [documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/CFProto.html). The example notebook illustrates this approach on the *adult* dataset, which contains a mixture of categorical and numerical features used to predict whether a person's income is above or below $50k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Note\n",
    "    \n",
    "To enable support for CounterfactualProto, you may need to run\n",
    "    \n",
    "```bash\n",
    "pip install alibi[tensorflow]\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.12.1\n",
      "Eager execution enabled:  True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(40) # suppress deprecation messages\n",
    "#tf.compat.v1.disable_v2_behavior() # disable TF2 behaviour as alibi code still relies on TF1 constructs\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from time import time\n",
    "from alibi.datasets import fetch_adult\n",
    "from alibi.explainers import CounterfactualProto\n",
    "from alibi.utils import ohe_to_ord, ord_to_ohe\n",
    "\n",
    "print('TF version: ', tf.__version__)\n",
    "print('Eager execution enabled: ', tf.executing_eagerly()) # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple, Dict, Callable\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from alibi.explainers import CounterfactualRLTabular, CounterfactualRL\n",
    "from alibi.datasets import fetch_adult\n",
    "from alibi.models.tensorflow import HeAE\n",
    "from alibi.models.tensorflow import Actor, Critic\n",
    "from alibi.models.tensorflow import ADULTEncoder, ADULTDecoder\n",
    "from alibi.explainers.cfrl_base import Callback\n",
    "from alibi.explainers.backends.cfrl_tabular import get_he_preprocessor, get_statistics, \\\n",
    "    get_conditional_vector, apply_category_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load adult dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fetch_adult` function returns a `Bunch` object containing the features, the targets, the feature names and a mapping of the categories in each categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = fetch_adult()\n",
    "data = adult.data\n",
    "target = adult.target\n",
    "feature_names = adult.feature_names\n",
    "category_map_tmp = adult.category_map\n",
    "target_names = adult.target_names\n",
    "\n",
    "\n",
    "# Separate columns in numerical and categorical.\n",
    "categorical_names = [adult.feature_names[i] for i in adult.category_map.keys()]\n",
    "categorical_ids = list(adult.category_map.keys())\n",
    "\n",
    "numerical_names = [name for i, name in enumerate(adult.feature_names) if i not in adult.category_map.keys()]\n",
    "numerical_ids = [i for i in range(len(adult.feature_names)) if i not in adult.category_map.keys()]\n",
    "\n",
    "# Split data into train and test\n",
    "X, Y = adult.data, adult.target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical standard scaler.\n",
    "num_transf = StandardScaler()\n",
    "\n",
    "# Define categorical one-hot encoder.\n",
    "cat_transf = OneHotEncoder(\n",
    "    categories=[range(len(x)) for x in adult.category_map.values()],\n",
    "    handle_unknown=\"ignore\"\n",
    ")\n",
    "\n",
    "# Define column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", cat_transf, categorical_ids),\n",
    "        (\"num\", num_transf, numerical_ids),\n",
    "    ],\n",
    "    sparse_threshold=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit preprocessor.\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Preprocess train and test dataset.\n",
    "X_train_ohe = preprocessor.transform(X_train)\n",
    "X_test_ohe = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=0.5, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=0.5, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=0.5, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select one of the below classifiers.\n",
    "clf = XGBClassifier(min_child_weight=0.5, max_depth=3, gamma=0.2)\n",
    "# clf = LogisticRegression(C=10)\n",
    "# clf = DecisionTreeClassifier(max_depth=10, min_samples_split=5)\n",
    "#clf = RandomForestClassifier(max_depth=15, min_samples_split=10, n_estimators=50)\n",
    "\n",
    "# Fit the classifier.\n",
    "clf.fit(X_train_ohe, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.870\n"
     ]
    }
   ],
   "source": [
    "# Define prediction function.\n",
    "predictor = lambda x: clf.predict_proba(preprocessor.transform(x))\n",
    "# Compute accuracy.\n",
    "acc = accuracy_score(y_true=Y_test, y_pred=predictor(X_test).argmax(axis=1))\n",
    "print(\"Accuracy: %.3f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define attribute types, required for datatype conversion.\n",
    "feature_types = {\"Age\": int, \"Capital Gain\": int, \"Capital Loss\": int, \"Hours per week\": int}\n",
    "\n",
    "# Define data preprocessor and inverse preprocessor. The invers preprocessor include datatype conversions.\n",
    "heae_preprocessor, heae_inv_preprocessor = get_he_preprocessor(X=X_train,\n",
    "                                                               feature_names=adult.feature_names,\n",
    "                                                               category_map=adult.category_map,\n",
    "                                                               feature_types=feature_types)\n",
    "\n",
    "# Define trainset\n",
    "trainset_input = heae_preprocessor(X_train).astype(np.float32)\n",
    "trainset_outputs = {\n",
    "    \"output_1\": trainset_input[:, :len(numerical_ids)]\n",
    "}\n",
    "\n",
    "for i, cat_id in enumerate(categorical_ids):\n",
    "    trainset_outputs.update({\n",
    "        f\"output_{i+2}\": X_train[:, cat_id]\n",
    "    })\n",
    "    \n",
    "trainset = tf.data.Dataset.from_tensor_slices((trainset_input, trainset_outputs))\n",
    "trainset = trainset.shuffle(1024).batch(128, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define autoencoder path and create dir if it doesn't exist.\n",
    "heae_path = os.path.join(\"tensorflow\", \"ADULT_autoencoder\")\n",
    "if not os.path.exists(heae_path):\n",
    "    os.makedirs(heae_path)\n",
    "\n",
    "# Define constants.\n",
    "EPOCHS = 50              # epochs to train the autoencoder\n",
    "HIDDEN_DIM = 128         # hidden dimension of the autoencoder\n",
    "LATENT_DIM = 15          # define latent dimension\n",
    "\n",
    "# Define output dimensions.\n",
    "OUTPUT_DIMS = [len(numerical_ids)]\n",
    "OUTPUT_DIMS += [len(adult.category_map[cat_id]) for cat_id in categorical_ids]\n",
    "\n",
    "# Define the heterogeneous auto-encoder.\n",
    "heae = HeAE(encoder=ADULTEncoder(hidden_dim=HIDDEN_DIM, latent_dim=LATENT_DIM),\n",
    "            decoder=ADULTDecoder(hidden_dim=HIDDEN_DIM, output_dims=OUTPUT_DIMS))\n",
    "\n",
    "# Define loss functions.\n",
    "he_loss = [keras.losses.MeanSquaredError()]\n",
    "he_loss_weights = [1.]\n",
    "\n",
    "# Add categorical losses.\n",
    "for i in range(len(categorical_names)):\n",
    "    he_loss.append(keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "    he_loss_weights.append(1./len(categorical_names))\n",
    "\n",
    "# Define metrics.\n",
    "metrics = {}\n",
    "for i, cat_name in enumerate(categorical_names):\n",
    "    metrics.update({f\"output_{i+2}\": keras.metrics.SparseCategoricalAccuracy()})\n",
    "    \n",
    "# Compile model.\n",
    "heae.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "             loss=he_loss,\n",
    "             loss_weights=he_loss_weights,\n",
    "             metrics=metrics)\n",
    "\n",
    "if len(os.listdir(heae_path)) == 0:\n",
    "    # Fit and save autoencoder.\n",
    "    heae.fit(trainset, epochs=EPOCHS)\n",
    "    heae.save(heae_path, save_format=\"tf\")\n",
    "else:\n",
    "    # Load the model.\n",
    "    heae = keras.models.load_model(heae_path, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define shuffled training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(s=0):\n",
    "    np.random.seed(s)\n",
    "    tf.random.set_seed(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "data_perm = np.random.permutation(np.c_[data, target])\n",
    "X = data_perm[:,:-1]\n",
    "y = data_perm[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 30000\n",
    "y_train, y_test = y[:idx], y[idx+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reorganize data so categorical features come first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[X[:, 1:8], X[:, 11], X[:, 0], X[:, 8:11]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust `feature_names` and `category_map` as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Workclass', 'Education', 'Marital Status', 'Occupation', 'Relationship', 'Race', 'Sex', 'Country', 'Age', 'Capital Gain', 'Capital Loss', 'Hours per week']\n"
     ]
    }
   ],
   "source": [
    "feature_names = feature_names[1:8] + feature_names[11:12] + feature_names[0:1] + feature_names[8:11]\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_map = {}\n",
    "for i, (_, v) in enumerate(category_map_tmp.items()):\n",
    "    category_map[i] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary with as keys the categorical columns and values the number of categories for each variable in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 9, 1: 7, 2: 4, 3: 9, 4: 6, 5: 5, 6: 2, 7: 11}\n"
     ]
    }
   ],
   "source": [
    "cat_vars_ord = {}\n",
    "n_categories = len(list(category_map.keys()))\n",
    "for i in range(n_categories):\n",
    "    cat_vars_ord[i] = len(np.unique(X[:, i]))\n",
    "print(cat_vars_ord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will apply one-hot encoding (OHE) on the categorical variables, we convert `cat_vars_ord` from the ordinal to OHE format. `alibi.utils.mapping` contains utility functions to do this. The keys in `cat_vars_ohe` now represent the first column index for each one-hot encoded categorical variable. This dictionary will later be used in the counterfactual explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 9, 9: 7, 16: 4, 20: 9, 29: 6, 35: 5, 40: 2, 42: 11}\n"
     ]
    }
   ],
   "source": [
    "cat_vars_ohe = ord_to_ohe(X, cat_vars_ord)[1]\n",
    "print(cat_vars_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale numerical features between -1 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = X[:, -4:].astype(np.float32, copy=False)\n",
    "xmin, xmax = X_num.min(axis=0), X_num.max(axis=0)\n",
    "rng = (-1., 1.)\n",
    "X_num_scaled = (X_num - xmin) / (xmax - xmin) * (rng[1] - rng[0]) + rng[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply OHE to categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = X[:, :-4].copy()\n",
    "ohe = OneHotEncoder(categories='auto', sparse=False).fit(X_cat)\n",
    "X_cat_ohe = ohe.transform(X_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine numerical and categorical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 57) (2560, 57)\n"
     ]
    }
   ],
   "source": [
    "X = np.c_[X_cat_ohe, X_num_scaled].astype(np.float32, copy=False)\n",
    "X_train, X_test = X[:idx, :], X[idx+1:, :]\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_ohe():\n",
    "    \n",
    "    x_in = Input(shape=(57,))\n",
    "    x = Dense(60, activation='relu')(x_in)\n",
    "    x = Dropout(.2)(x)\n",
    "    x = Dense(60, activation='relu')(x)\n",
    "    x = Dropout(.2)(x)\n",
    "    x = Dense(60, activation='relu')(x)\n",
    "    x = Dropout(.2)(x)\n",
    "    x_out = Dense(2, activation='softmax')(x)\n",
    "    \n",
    "    nn = Model(inputs=x_in, outputs=x_out)\n",
    "    nn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 57)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 60)                3480      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 122       \n",
      "=================================================================\n",
      "Total params: 10,922\n",
      "Trainable params: 10,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2d7cc5e410>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed()\n",
    "nn = nn_ohe()\n",
    "nn.summary()\n",
    "nn.fit(X_train, to_categorical(y_train), batch_size=256, epochs=30, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate counterfactual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_test_ohe[0].reshape((1,) + X_test_ohe[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize counterfactual parameters. The feature perturbations are applied in the numerical feature space, after transforming the categorical variables to numerical features. As a result, the dimensionality and values of `feature_range` are defined in the numerical space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = X.shape\n",
    "beta = .01\n",
    "c_init = 1.\n",
    "c_steps = 5\n",
    "max_iterations = 500\n",
    "rng = (-1., 1.)  # scale features between -1 and 1\n",
    "rng_shape = (1,) + data.shape[1:]\n",
    "feature_range = ((np.ones(rng_shape) * rng[0]).astype(np.float32), \n",
    "                 (np.ones(rng_shape) * rng[1]).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]],\n",
       "       dtype=float32),\n",
       " array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize explainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(s=0):\n",
    "    np.random.seed(s)\n",
    "    tf.random.set_seed(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars_ord = {key: len(val) for key, val in adult.category_map.items()}\n",
    "cat_vars_ohe = ord_to_ohe(X, cat_vars_ord)[1]\n",
    "tf.compat.v1.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 9, 10: 7, 17: 4, 21: 9, 30: 6, 36: 5, 41: 2, 46: 11}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_vars_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(x):\n",
    "    return clf.predict_proba(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "cf = CounterfactualProto(predictor,\n",
    "                         shape,\n",
    "                         beta=beta,\n",
    "                         cat_vars=cat_vars_ohe,\n",
    "                         ohe=True,  # OHE flag\n",
    "                         max_iterations=max_iterations,\n",
    "                         feature_range=feature_range,\n",
    "                         c_init=c_init,\n",
    "                         c_steps=c_steps\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit explainer. `d_type` refers to the distance metric used to convert the categorical to numerical values. Valid options are `abdm`, `mvdm` and `abdm-mvdm`. `abdm` infers the distance between categories of the same variable from the context provided by the other variables. This requires binning of the numerical features as well. `mvdm` computes the distance using the model predictions, and `abdm-mvdm` combines both methods. More info on both distance measures can be found in the [documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/CFProto.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26048, 57)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_ohe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabdm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc_perc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m75\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\alibi\\explainers\\cfproto.py:720\u001b[0m, in \u001b[0;36mCounterfactualProto.fit\u001b[1;34m(self, train_data, trustscore_kwargs, d_type, w, disc_perc, standardize_cat_vars, smooth, center, update_feature_range)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cat:  \u001b[38;5;66;03m# compute distance metrics for categorical variables\u001b[39;00m\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mohe:  \u001b[38;5;66;03m# convert OHE to ordinal encoding\u001b[39;00m\n\u001b[1;32m--> 720\u001b[0m         train_data_ord, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_vars_ord \u001b[38;5;241m=\u001b[39m \u001b[43mohe_to_ord\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    722\u001b[0m         train_data_ord, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_vars_ord \u001b[38;5;241m=\u001b[39m train_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_vars\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\alibi\\utils\\mapping.py:149\u001b[0m, in \u001b[0;36mohe_to_ord\u001b[1;34m(X_ohe, cat_vars_ohe)\u001b[0m\n\u001b[0;32m    147\u001b[0m v \u001b[38;5;241m=\u001b[39m cat_vars_ohe[c]\n\u001b[0;32m    148\u001b[0m X_ohe_c \u001b[38;5;241m=\u001b[39m X_ohe[:, c:c \u001b[38;5;241m+\u001b[39m v]\n\u001b[1;32m--> 149\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39msum(X_ohe_c, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum()) \u001b[38;5;241m==\u001b[39m n\n\u001b[0;32m    150\u001b[0m X_ord_c \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(X_ohe_c, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    151\u001b[0m cat_vars_ord[\u001b[38;5;28mlen\u001b[39m(X_list)] \u001b[38;5;241m=\u001b[39m v\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cf.fit(X_train_ohe, d_type='abdm', disc_perc=[25, 50, 75]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 26045 26046 26047]\n"
     ]
    }
   ],
   "source": [
    "ohe_sums = np.sum(X_train_ohe, axis=1)\n",
    "invalid_rows = np.where((ohe_sums != 1))[0]\n",
    "print(invalid_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the transformation from the categorical to numerical values for each category. The example below shows that the **Education** feature is ordered from *High School Dropout* to having obtained a *Doctorate* degree. As a result, if we perturb an instance representing a person that has obtained a *Bachelors* degree, the nearest perturbations will result in a counterfactual instance with either a *Masters* or an *Associates* degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar(dist, cols, figsize=(10,4)):\n",
    "    dist = dist.reshape(dist.shape[0])\n",
    "    idx = np.argsort(dist)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plt.bar(cols[idx], dist[idx])\n",
    "    print(cols[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dropout' 'High School grad' 'Associates' 'Bachelors' 'Masters'\n",
      " 'Prof-School' 'Doctorate']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cat = 'Education'\n",
    "idx = feature_names.index(cat)\n",
    "plot_bar(cf.d_abs[idx], np.array(category_map[idx]), figsize=(20,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = cf.explain(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to more clearly describe explanations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_instance(X, explanation, eps=1e-2):\n",
    "    print('Original instance: {}  -- proba: {}'.format(target_names[explanation.orig_class],\n",
    "                                                       explanation.orig_proba[0]))\n",
    "    print('Counterfactual instance: {}  -- proba: {}'.format(target_names[explanation.cf['class']],\n",
    "                                                             explanation.cf['proba'][0]))\n",
    "    print('\\nCounterfactual perturbations...')\n",
    "    print('\\nCategorical:')\n",
    "    X_orig_ord = ohe_to_ord(X, cat_vars_ohe)[0]\n",
    "    X_cf_ord = ohe_to_ord(explanation.cf['X'], cat_vars_ohe)[0]\n",
    "    delta_cat = {}\n",
    "    for i, (_, v) in enumerate(category_map.items()):\n",
    "        cat_orig = v[int(X_orig_ord[0, i])]\n",
    "        cat_cf = v[int(X_cf_ord[0, i])]\n",
    "        if cat_orig != cat_cf:\n",
    "            delta_cat[feature_names[i]] = [cat_orig, cat_cf]\n",
    "    if delta_cat:\n",
    "        for k, v in delta_cat.items():\n",
    "            print('{}: {}  -->   {}'.format(k, v[0], v[1]))\n",
    "    print('\\nNumerical:')\n",
    "    delta_num = X_cf_ord[0, -4:] - X_orig_ord[0, -4:]\n",
    "    n_keys = len(list(cat_vars_ord.keys()))\n",
    "    for i in range(delta_num.shape[0]):\n",
    "        if np.abs(delta_num[i]) > eps:\n",
    "            print('{}: {:.2f}  -->   {:.2f}'.format(feature_names[i+n_keys],\n",
    "                                            X_orig_ord[0,i+n_keys],\n",
    "                                            X_cf_ord[0,i+n_keys]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original instance: <=50K  -- proba: [0.70744723 0.29255277]\n",
      "Counterfactual instance: >50K  -- proba: [0.37736374 0.62263626]\n",
      "\n",
      "Counterfactual perturbations...\n",
      "\n",
      "Categorical:\n",
      "Education: Associates  -->   Bachelors\n",
      "\n",
      "Numerical:\n"
     ]
    }
   ],
   "source": [
    "describe_instance(X, explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By obtaining a higher level of education the income is predicted to be above $50k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the categorical distance metric\n",
    "\n",
    "Instead of `abdm`, we now use `mvdm` as our distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original instance: <=50K  -- proba: [0.70744723 0.29255277]\n",
      "Counterfactual instance: >50K  -- proba: [0.38161737 0.61838263]\n",
      "\n",
      "Counterfactual perturbations...\n",
      "\n",
      "Categorical:\n",
      "Education: Associates  -->   Bachelors\n",
      "\n",
      "Numerical:\n"
     ]
    }
   ],
   "source": [
    "set_seed()\n",
    "cf.fit(X_train, d_type='mvdm')\n",
    "explanation = cf.explain(X)\n",
    "describe_instance(X, explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same conclusion hold using a different distance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use k-d trees to build prototypes\n",
    "\n",
    "We can also use *k-d trees* to build class prototypes to guide the counterfactual to nearby instances in the counterfactual class as described in [Interpretable Counterfactual Explanations Guided by Prototypes](https://arxiv.org/abs/1907.02584). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_kdtree = True\n",
    "theta = 10.  # weight of prototype loss term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize, fit and explain instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original instance: <=50K  -- proba: [0.5211548  0.47884512]\n",
      "Counterfactual instance: >50K  -- proba: [0.49958408 0.500416  ]\n",
      "\n",
      "Counterfactual perturbations...\n",
      "\n",
      "Categorical:\n",
      "\n",
      "Numerical:\n",
      "Age: -0.53  -->   -0.51\n"
     ]
    }
   ],
   "source": [
    "set_seed()\n",
    "X = X_test[7].reshape((1,) + X_test[0].shape)\n",
    "cf = CounterfactualProto(nn,\n",
    "                         shape,\n",
    "                         beta=beta,\n",
    "                         theta=theta,\n",
    "                         cat_vars=cat_vars_ohe,\n",
    "                         ohe=True,\n",
    "                         use_kdtree=use_kdtree,\n",
    "                         max_iterations=max_iterations,\n",
    "                         feature_range=feature_range,\n",
    "                         c_init=c_init,\n",
    "                         c_steps=c_steps\n",
    "                        )\n",
    "cf.fit(X_train, d_type='abdm')\n",
    "explanation = cf.explain(X)\n",
    "describe_instance(X, explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By slightly increasing the age of the person the income would be predicted to be above $50k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use an autoencoder to build prototypes\n",
    "\n",
    "Another option is to use an autoencoder to guide the perturbed instance to the counterfactual class. We define and train the autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_model():\n",
    "    # encoder\n",
    "    x_in = Input(shape=(57,))\n",
    "    x = Dense(60, activation='relu')(x_in)\n",
    "    x = Dense(30, activation='relu')(x)\n",
    "    x = Dense(15, activation='relu')(x)\n",
    "    encoded = Dense(10, activation=None)(x)\n",
    "    encoder = Model(x_in, encoded)\n",
    "    \n",
    "    # decoder\n",
    "    dec_in = Input(shape=(10,))\n",
    "    x = Dense(15, activation='relu')(dec_in)\n",
    "    x = Dense(30, activation='relu')(x)\n",
    "    x = Dense(60, activation='relu')(x)\n",
    "    decoded = Dense(57, activation=None)(x)\n",
    "    decoder = Model(dec_in, decoded)\n",
    "    \n",
    "    # autoencoder = encoder + decoder\n",
    "    x_out = decoder(encoder(x_in))\n",
    "    autoencoder = Model(x_in, x_out)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 57)]              0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 10)                5935      \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 57)                5982      \n",
      "=================================================================\n",
      "Total params: 11,917\n",
      "Trainable params: 11,917\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2d783aff90>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed()\n",
    "ae, enc, dec = ae_model()\n",
    "ae.summary()\n",
    "ae.fit(X_train, X_train, batch_size=128, epochs=100, validation_data=(X_test, X_test), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights for the autoencoder and prototype loss terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = .1  # L1\n",
    "gamma = 10.  # autoencoder\n",
    "theta = .1  # prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize, fit and explain instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original instance: >50K  -- proba: [0.48656026 0.5134398 ]\n",
      "Counterfactual instance: <=50K  -- proba: [0.71456206 0.28543794]\n",
      "\n",
      "Counterfactual perturbations...\n",
      "\n",
      "Categorical:\n",
      "Education: High School grad  -->   Dropout\n",
      "\n",
      "Numerical:\n"
     ]
    }
   ],
   "source": [
    "set_seed()\n",
    "X = X_test[19].reshape((1,) + X_test[0].shape)\n",
    "cf = CounterfactualProto(nn,\n",
    "                         shape,\n",
    "                         beta=beta,\n",
    "                         enc_model=enc,\n",
    "                         ae_model=ae,\n",
    "                         gamma=gamma,\n",
    "                         theta=theta,\n",
    "                         cat_vars=cat_vars_ohe,\n",
    "                         ohe=True,\n",
    "                         max_iterations=max_iterations,\n",
    "                         feature_range=feature_range,\n",
    "                         c_init=c_init,\n",
    "                         c_steps=c_steps\n",
    "                        )\n",
    "cf.fit(X_train, d_type='abdm')\n",
    "explanation = cf.explain(X)\n",
    "describe_instance(X, explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black box model with k-d trees\n",
    "\n",
    "Now we assume that we only have access to the model's prediction function and treat it as a black box. The k-d trees are again used to define the prototypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_kdtree = True\n",
    "theta = 10.  # weight of prototype loss term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize, fit and explain instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original instance: >50K  -- proba: [0.20676644 0.7932335 ]\n",
      "Counterfactual instance: <=50K  -- proba: [0.5048416  0.49515834]\n",
      "\n",
      "Counterfactual perturbations...\n",
      "\n",
      "Categorical:\n",
      "\n",
      "Numerical:\n",
      "Age: -0.15  -->   -0.19\n",
      "Hours per week: -0.20  -->   -0.51\n"
     ]
    }
   ],
   "source": [
    "set_seed()\n",
    "\n",
    "X = X_test[24].reshape((1,) + X_test[0].shape)\n",
    "\n",
    "# define predict function\n",
    "predict_fn = lambda x: nn.predict(x)\n",
    "\n",
    "cf = CounterfactualProto(predict_fn,\n",
    "                         shape,\n",
    "                         beta=beta,\n",
    "                         theta=theta,\n",
    "                         cat_vars=cat_vars_ohe,\n",
    "                         ohe=True,\n",
    "                         use_kdtree=use_kdtree,\n",
    "                         max_iterations=max_iterations,\n",
    "                         feature_range=feature_range,\n",
    "                         c_init=c_init,\n",
    "                         c_steps=c_steps\n",
    "                        )\n",
    "cf.fit(X_train, d_type='abdm')\n",
    "explanation = cf.explain(X)\n",
    "describe_instance(X, explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the person was younger and worked less, he or she would have a predicted income below $50k."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
